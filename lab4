import math
from os import error 
import random
import gzip
import numpy as np

class Layer:
    neurons = 0
    values = []
    weights = []
    biases = []
    stimulation = []
    activationFunc = lambda x: x
    derivative = lambda x: x

    def __init__(self, neurons):
        self.neurons = neurons

def softMax(vector):
    vector = np.asarray(vector).flatten()
    denominator = sum([math.e ** x for x in vector])
    return np.transpose(np.matrix([(math.e ** x) / denominator for x in vector]))

def multiplyWeights(weights, values):
    answer = []
    for row in weights:
        sum = 0
        for i in range(0, len(row)):
            sum += row[i] * values[i]
        answer.append(sum)
    return answer

def addBiases(stimulations, biases):
    answer = []
    for i in range (0, len(stimulations)):
        answer.append(biases[i] + stimulations[i])
    return answer

def calculateCompleteStimulation(weights, values, biases):
    value =  (weights @ values) + biases
    return value

def sigmoidal(vector):
    f = lambda vec: 1 / (1 + (math.e ** vec[0]))
    a = np.transpose(np.matrix(np.apply_along_axis(f, 1, np.asarray(vector))))
    return a

def calculate(network, inputs):

    def propagate(layerNumber, values):
        if layerNumber == len(network) :
            return values
        layer = network[layerNumber]
        completeStimulation = calculateCompleteStimulation(layer.weights, values, layer.biases)
        totalActivation = layer.activationFunc(completeStimulation)
        layer.stimulation = completeStimulation
        layer.values = totalActivation
        return propagate(layerNumber + 1, totalActivation)

    outputActivation = propagate(1, inputs)
    return outputActivation

def generateWeights(neurons, nextNeurons):
    answer = []
    for i in range(0, nextNeurons):
        answer.append([random.random() * 0.00001 for _ in range(0, neurons)])    
    return np.matrix(answer)


def getTrainingSet():
    file = gzip.open("train-images-idx3-ubyte.gz", "r")
    bytes = file.read()
    images = int.from_bytes(bytes[4:8], byteorder='big', signed=False)
    rows = int.from_bytes(bytes[8:12], byteorder='big', signed=False)
    columns = int.from_bytes(bytes[12:16], byteorder='big', signed=False)
    pixelsInImage = rows * columns
    imagesData = []
    for i in range(0, images):
        data = bytes[16 + i * pixelsInImage:16 + (i + 1) * pixelsInImage]
        imagesData.append([int(b) for b in data])
    labels = gzip.open("train-labels-idx1-ubyte.gz", "r").read()
    labelsData = labels[8: images + 8]
    return [(imagesData[i], labelsData[i]) for i in range(0, images)]

def bytesToNumber():
    int(bytes.encode('hex'), 16)

def sigmoidalDerivative(vector):
    vector = np.asarray(vector).flatten()
    answer = []
    for v in vector:
        val = 1 / (1 + math.e ** -v)
        answer.append(1 / (1 + math.e ** (1 - val)))
    return answer

def softPlus(vector):
    return [math.log(1 + math.e ** v) for v in vector]

def ReLuDerivative(vector):
    return sigmoidal(vector)

def adjustWeights():
    pass

def train(network, trainSets, batchSize, alpha):
    i = 0
    while i < len(trainSets):
        for trainSet in range(0, batchSize):
            if i == len(trainSets):
                break
            
            trainSet = trainSets[i]
            inputs = [x[0] for x in trainSet]
            answers = [x[1] for x in trainSet]
            result = calculate(network, np.transpose(np.matrix(inputs)))
            adjustments = getWeightAdjustment(network, answers)
            adjustWeights(network, adjustments, alpha)
            i += 1
    


def getWeightAdjustment(network, answers):
    answers = np.transpose(np.matrix(answers))
    sigmas = []

    def propagate(layerNumber, previousSigma):
        if layerNumber == -1:
            return
        nonlocal sigmas
        print(previousSigma)
        layer = network[layerNumber]
        weights = np.transpose(network[layerNumber + 1].weights)
        derived = layer.derivative(layer.stimulation)
        aaa = weights @ previousSigma
        layerSigma = (weights @ previousSigma) * derived
        sigmas.append(layerSigma)
        propagate(layerNum - 1, layerSigma)

    layerNum = len(network) - 1
    layer = network[layerNum]
    #firstPart = answers @ np.linalg.inv(layer.values) 
    sigma = -(answers - layer.values)
    #sigma = firstPart * secondPart
    sigmas.append(sigma)
    propagate(layerNum - 1, sigma)
    return sigmas

def prepare(hiddenNumberNeurons, function, derivative):
    network = [ Layer(x) for x in [hiddenNumberNeurons[0], hiddenNumberNeurons[1], hiddenNumberNeurons[2], 10]]
    for i in range(1, len(network)):
        layer = network[i]
        layer.activationFunc = softMax if i == len(network) - 1 else function
        layer.derivative = derivative
        layer.biases = np.transpose(np.matrix([random.random() * 0.1 for _ in range(0, layer.neurons)]))
        layer.weights = generateWeights(network[i - 1].neurons, layer.neurons)

    images = getTrainingSet()
    imageValues = [data[0] for data in images]

def test():


if __name__ == '__main__':
    network = [ Layer(x) for x in [4, 5, 8, 10]]
    for i in range(1, len(network)):
        layer = network[i]
        layer.activationFunc = softMax if i == len(network) - 1 else sigmoidal
        layer.derivative = sigmoidalDerivative
        layer.biases = np.transpose(np.matrix([random.random() * 0.1 for _ in range(0, layer.neurons)]))
        layer.weights = generateWeights(network[i - 1].neurons, layer.neurons)

    images = getTrainingSet()
    imageValues = [data[0] for data in images]
    result = calculate(network, np.transpose(np.matrix(imageValues[0][:4])))
    #print(result)
    sigmas = adjustWeights(network, [1] + [0] * 9)
    print(sigmas)
