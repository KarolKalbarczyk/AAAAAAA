import math
from os import error 
import random
import gzip
import numpy as np
import warnings
import time
import multiprocessing
from joblib import Parallel, delayed

maxVec = 0

class Layer:
    neurons = 0
    values = []
    weights = []
    biases = []
    stimulation = []
    activationFunc = lambda x: x
    derivative = lambda x: x

    def __init__(self, neurons):
        self.neurons = neurons

def softMax(vector):
    vector = np.asarray(vector).flatten()
    denominator = sum([math.e ** x for x in vector])
    return np.transpose(np.matrix([(math.e ** x) / denominator for x in vector]))

def calculateCompleteStimulation(weights, values, biases):
    return (weights @ values) + biases

def tanh(vector):
    vec = np.asarray(vector).flatten()
    return np.transpose(np.matrix([(math.e ** v - math.e ** -v) / (math.e ** v + math.e ** -v) if v < 8 else 1 for v in vec]))

def tanhDerivative(vector):
    tan = tanh(vector)
    return 1 - (np.multiply(tan, tan))    
    
def sigmoidalDerivative(vector):
    val = sigmoidal(vector)
    return sigmoidal(1 - val)

def softPlus(vector):
    vec = np.asarray(vector).flatten()
    return np.transpose(np.matrix([math.log(1 + math.e ** v) for v in vec]))

def ReLu(vector):
    vec = np.asarray(vector).flatten()
    return np.transpose(np.matrix([0 if v < 0 else (4 if v > 4 else v) for v in vec]))

def ReLuDerivative(vector):
    return sigmoidal(vector)

def sigmoidal(vector):
    return 1 / (1 + np.exp(-vector))
    array = np.asarray(vector).flatten()
    result = []
    for x in array:
        if x > 10:
            result.append(1)
        elif x < -10:
            result.append(0)
        else:
            result.append(1 / (1 + math.e ** -x))
    return np.transpose(np.matrix(result))

def calculate(network, inputs):
    def propagate(layerNumber, values):
        if layerNumber == len(network) :
            return values

        layer = network[layerNumber]
        completeStimulation = calculateCompleteStimulation(layer.weights, values, layer.biases)
        totalActivation = layer.activationFunc(completeStimulation)
        layer.stimulation = completeStimulation
        layer.values = totalActivation
        return propagate(layerNumber + 1, totalActivation)

    network[0].values = inputs
    outputActivation = propagate(1, inputs)
    return outputActivation

def generateWeights(neurons, nextNeurons, std):
    answer = []
    for _ in range(0, nextNeurons):
        list = np.random.normal(0, std, neurons).tolist()
        answer.append(list)    
    return np.matrix(answer)

def getTrainingSet(name, labelName):
    file = gzip.open(name, "r")
    bytes = file.read()
    images = int.from_bytes(bytes[4:8], byteorder='big', signed=False)
    rows = int.from_bytes(bytes[8:12], byteorder='big', signed=False)
    columns = int.from_bytes(bytes[12:16], byteorder='big', signed=False)
    pixelsInImage = rows * columns
    imagesData = []
    for i in range(0, images):
        data = bytes[16 + i * pixelsInImage:16 + (i + 1) * pixelsInImage]
        imagesData.append([int(b) / 2550 for b in data])
    labels = gzip.open(labelName, "r").read()
    labelsData = labels[8: images + 8]
    return [(imagesData[i], labelsData[i]) for i in range(0, images)]


def train(network, trainSets2, testingSet, batchSize, alpha, stoppingTreshold):
    maxAcc = 0
        
    accValid = 0
    epochCount = 0
    while accValid < 0.85 and epochCount < 30:
        random.shuffle(trainSets2)
        trainSets = trainSets2[:10000]
        i = 0
        accTest = 0
        savedValues = []
        while i < len(trainSets):
            sigmas = []
            #print("newbatch")
            for _ in range(0, batchSize):
                if i == len(trainSets):
                    break
                inputs, answer = trainSets[i]
                result = calculate(network, np.transpose(np.matrix(inputs)))
                answers = [0 for _ in range(0, 10)]
                answers[answer - 1] = 1
                adjustments = getWeightAdjustment(network, answers)
                adjustments.reverse()
                #adjustWeights(network, [adjustments], alpha)
                sigmas.append(adjustments)
                i += 1
                accTest += 1 if np.argmax(result) == answer - 1 else 0

                #print(np.argmax(result))
                
                # if np.argmax(result) == answer - 1:
                #     print(np.argmax(result))
                #     print(answer - 1)

            adjustWeights(network, sigmas, alpha, batchSize)
            
        for test in testingSet:
            inputs, answer = test
            result = calculate(network, np.transpose(np.matrix(inputs)))
            accValid += 1 if np.argmax(result) == answer - 1 else 0
        accValid = accValid / len(testingSet)
        if (accValid) > maxAcc:
            maxAcc = (accValid / len(testingSet))
            savedValues = [(x.weights, x.biases) for x in network]
        
        if  (accTest / len(trainSets)) - accValid > stoppingTreshold:
            for i in range(0, len(network)):
                layer = network[i] 
                layer.weights, layer.biases = savedValues[i]

        epochCount += 1

    return epochCount
    

def getWeightAdjustment(network, answers):
    answers = np.transpose(np.matrix(answers))
    sigmas = []

    def propagate(layerNumber, previousSigma):
        if layerNumber == 0:
            return
        nonlocal sigmas
        layer = network[layerNumber]
        weights = np.transpose(network[layerNumber + 1].weights)
        derived = layer.derivative(layer.values)
        layerSigma = np.multiply((weights @ previousSigma), derived)
        layerSigmaAdjustments = layerSigma @ np.transpose(network[layerNumber - 1].values)
        sigmas.append((layerSigma, layerSigmaAdjustments))
        propagate(layerNumber - 1, layerSigma)

    layerNum = len(network) - 1
    layer = network[layerNum]
    sigma = -(answers - layer.values)
    sigmaAdjustment = sigma @ np.transpose(network[layerNum - 1].values)
    sigmas.append((sigma, sigmaAdjustment))
    propagate(layerNum - 1, sigma)
    return sigmas

def adjustWeights(network, sigmas, alpha, batchSize):
    for i in range(1, len(network)):
        layer = network[i]
        first = sigmas[0][i - 1][1]
        biases = sigmas[0][i - 1][0]
        for j in range(1, len(sigmas)):
            first = first + sigmas[j][i - 1][1]
            biases = biases + sigmas[j][i - 1][0]
        layer.weights = layer.weights - ((alpha / batchSize) * first)
        layer.biases = layer.biases - ((alpha / batchSize) * biases)


def test(network, testingSet):
    accValid = 0
    for test in testingSet:
        inputs, answer = test
        result = calculate(network, np.transpose(np.matrix(inputs)))
        accValid += 1 if np.argmax(result) == answer - 1 else 0
    return accValid / len(testingSet)


def prepare(hiddenNumberNeurons, functions, derivatives, alpha, batchSize, stoppingTreshold, std, i, images, testingImages):
    np.random.seed(i)
    network = [ Layer(x) for x in hiddenNumberNeurons + [10]]
    for i in range(1, len(network)):
        layer = network[i]
        layer.activationFunc = softMax if i == len(network) - 1 else functions[i]
        layer.derivative = None if i == len(network) - 1 else derivatives[i]
        layer.biases = np.transpose(np.matrix((np.random.normal(0, std, layer.neurons)).tolist()))
        layer.weights = generateWeights(network[i - 1].neurons, layer.neurons, std)

    training = images[:59000]
    testing = images[59000:]
    epochCount = train(network, training, testing, batchSize, alpha, stoppingTreshold)
    performance = test(network, testingImages)
    return epochCount, performance


if __name__ == '__main__':
    activations = [tanh, tanh, tanh]
    images = getTrainingSet("train-images-idx3-ubyte.gz", "train-labels-idx1-ubyte.gz")
    testingImages = getTrainingSet("t10k-images-idx3-ubyte.gz", "t10k-labels-idx1-ubyte.gz")

    derivatives = [tanhDerivative, tanhDerivative, tanhDerivative]
    reDerivatives = [ReLuDerivative, ReLuDerivative, ReLuDerivative]
    relus = [ReLu, ReLu, ReLu]
    alphas = [0.02, 0.05, 0.1, 0.2, 0.5, 1, 5]
    for a in alphas:
        avgE = 0
        avgP = 0
        for i in range(0, 3):
            e, p = prepare([784, 100], activations, derivatives, a, 100, 0.5, 0.1, i, images, testingImages)
            avgE += e
            avgP += p
        print(f"alpha: {a} {(avgE / 3, avgP / 3)}")

    for s in [0.05, 0.1, 0.2, 0.5, 1, 2, 5]:
        avgE = 0
        avgP = 0
        for i in range(0, 3):
            e, p = prepare([784, 100], activations, derivatives, 0.5, 100, 0.5, s, i, images, testingImages)
            avgE += e
            avgP += p
        print(f"std: {s} {(avgE / 3, avgP / 3)}")
    
    size = [5, 10, 20, 50, 100, 200, 500, 1000]
    for s in size:
        avgE = 0
        avgP = 0
        for i in range(0, 3):
            e, p = prepare([784, s], activations, derivatives, 0.5, 100, 0.5, 0.1, i, images, testingImages)
            avgE += e
            avgP += p
        print(f"size: s {(avgE / 3, avgP / 3)}")

    batch = [1, 2, 5, 10, 20, 50, 100, 250, 1000]
    for b in batch:
        avgE = 0
        avgP = 0
        for i in range(0, 3):
            e, p = prepare([784, 100], activations, derivatives, 0.5, b, 0.5, 0.1, i, images, testingImages)
            avgE += e
            avgP += p
        print(f"batch: {b} {(avgE / 3, avgP / 3)}")
    
    network = [ Layer(x) for x in [784, 5, 8, 10]]
    for i in range(1, len(network)):
        layer = network[i]
        layer.activationFunc = softMax if i == len(network) - 1 else sigmoidal
        layer.derivative = sigmoidalDerivative
        layer.biases = np.transpose(np.matrix((np.random.normal(0, 0.1, layer.neurons)).tolist()))
        layer.weights = generateWeights(network[i - 1].neurons, layer.neurons)

    # a = generateWeights(5, 4, None)
    # print(a)

    # images = getTrainingSet()[:54000]
    # testing = getTrainingSet()[54000:]
    # train(network, images, testing, 100, 0.8)

    # imageValues = [data[0] for data in images]
    # print(len(imageValues[0]))
    # result = calculate(network, np.transpose(np.matrix(imageValues[0])))
    # print(len(network))
    
    # #print(result)
    # sigmas = getWeightAdjustment(network, [1] + [0] * 9)
    # sigmas.reverse()
    # adjustWeights(network, [sigmas], 0.2)
    
    #for r in network:
        #print(r.weights)
